{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN13BfVtixN8z8m9ACs+fjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasef-c/emotion_rec/blob/main/DATA_PRE_PROCESSING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATASET COLLECTION**\n",
        "\n",
        "The code below was used to extract data from the following datasets:\n",
        "\n",
        "Toronto Emotional Speech Set Data [(TESS)](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)\n",
        "\n",
        "Ryerson Audio-Visual Database of Emotional Speech and Song[(RAVDESS)](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)\n",
        "\n",
        "Crowd Sourced Emotional Multimodal Actors Dataset [(CREMA-D)](https://www.kaggle.com/datasets/ejlok1/cremad)\n",
        "\n",
        "Interactive Emotional Dyadic Motion Capture [(IEMOCAP)](https://www.kaggle.com/datasets/columbine/iemocap)\n",
        "\n"
      ],
      "metadata": {
        "id": "2T5y8XHQ3LCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "# from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "from tensorflow.keras.layers import (Dense, Bidirectional, ELU,\n",
        "                          Dropout, LeakyReLU, Conv1D, BatchNormalization)\n",
        "from keras.models import Sequential\n",
        "# from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Set seed for reproducability\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "t_start = time.time()\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "x1w5G5ul4cZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **TESS DATA COLLECTION**\n"
      ],
      "metadata": {
        "id": "_Dizspjj4jX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESS\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to your directory containing audio files\n",
        "#Change to your respective directory after downloading TESS or using kaggle to import\n",
        "\n",
        "directory_path = 'D:\\Documents\\MASC\\Emo_rec_001\\TESS Toronto emotional speech set data'\n",
        "\n",
        "# Filter filenames that start with '03-01' and end with language code '01'\n",
        "T_filtered_filenames = []\n",
        "T_emotion_labels = []\n",
        "T_labels = []\n",
        "T_fname = []\n",
        "\n",
        "# Mapping of emotion codes to corresponding emotions\n",
        "EMOTIONS  = {\n",
        "    0: 'neutral',\n",
        "    1: 'happy',\n",
        "    2: 'sad',\n",
        "    3: 'angry',\n",
        "    4: 'fearful',\n",
        "    5: 'disgust',\n",
        "}\n",
        "\n",
        "Map2Num = {\n",
        "    'neutral': 0,\n",
        "    'happy': 1,\n",
        "    'sad': 2,\n",
        "    'angry': 3,\n",
        "    'fear': 4,\n",
        "    'disgust': 5\n",
        "}\n",
        "\n",
        "\n",
        "def traverse_directories(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            full_file_path = os.path.join(root, filename)\n",
        "            T_emotion = str(filename.split('_')[2])\n",
        "            T_emotion = str(T_emotion.split('.')[0])\n",
        "            if T_emotion != 'ps':\n",
        "                T_filtered_filenames.append(full_file_path)\n",
        "                T_fname.append(filename)\n",
        "                T_label = Map2Num.get(T_emotion, 'Unknown')\n",
        "                T_labels.append(T_label)\n",
        "                T_emotion_labels.append(T_emotion)\n",
        "\n",
        "# Traverse through all subdirectories and extract file paths of English files\n",
        "traverse_directories(directory_path)"
      ],
      "metadata": {
        "id": "_tKlOPKq4nkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREMA-D**"
      ],
      "metadata": {
        "id": "gHr0BtzO48Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CREMA\n",
        "# Path to your directory containing audio files\n",
        "directory_path = 'D:\\Documents\\MASC\\Emo_rec_001\\CREMA-D\\AudioWAV'\n",
        "\n",
        "\n",
        "C_filenames = []\n",
        "C_emotion_labels = []\n",
        "C_labels = []\n",
        "C_fname = []\n",
        "C_filtered_filenames = []\n",
        "\n",
        "# Mapping of emotion codes to corresponding emotions\n",
        "EMOTIONS  = {\n",
        "    0: 'neutral',\n",
        "    1: 'happy',\n",
        "    2: 'sad',\n",
        "    3: 'angry',\n",
        "    4: 'fear',\n",
        "    5: 'disgust',\n",
        "}\n",
        "\n",
        "\n",
        "num_mapping = {\n",
        "    'NEU': 0,\n",
        "    'HAP': 1,\n",
        "    'SAD': 2,\n",
        "    'ANG': 3,\n",
        "    'FEA': 4,\n",
        "    'DIS': 5\n",
        "}\n",
        "\n",
        "\n",
        "# Function to filter English files and traverse directories\n",
        "def traverse_directories2(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            full_file_path = os.path.join(root, filename)\n",
        "            C_emotion = str(filename.split('_')[2])\n",
        "            C_filtered_filenames.append(full_file_path)\n",
        "            C_fname.append(filename)\n",
        "            C_label = num_mapping.get(C_emotion, 'Unknown')\n",
        "            C_labels.append(C_label)\n",
        "            C_Aemotion = EMOTIONS.get(C_label, 'Unknown')\n",
        "            C_emotion_labels.append(C_Aemotion)\n",
        "            # print(f\"File Path: {full_file_path}, Emotion: {emotion}\")\n",
        "\n",
        "# Traverse through all subdirectories and extract file paths of English files\n",
        "traverse_directories2(directory_path)"
      ],
      "metadata": {
        "id": "cj6i0Qpy5AZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAVDESS**"
      ],
      "metadata": {
        "id": "UpCu6RgG5H1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## RAVDESS\n",
        "# Path to your directory containing audio files\n",
        "directory_path = 'D:\\Documents\\MASC\\Emo_rec_001\\RAVDESS'\n",
        "\n",
        "# Filter filenames that start with '03-01' and end with language code '01'\n",
        "R_filenames = []\n",
        "R_emotion_labels = []\n",
        "R_labels = []\n",
        "R_fname = []\n",
        "R_filtered_filenames = []\n",
        "\n",
        "# Mapping of emotion codes to corresponding emotions\n",
        "EMOTIONS  = {\n",
        "    0: 'neutral',\n",
        "    1: 'happy',\n",
        "    2: 'sad',\n",
        "    3: 'angry',\n",
        "    4: 'fear',\n",
        "    5: 'disgust',\n",
        "}\n",
        "\n",
        "\n",
        "# Function to filter English files and traverse directories\n",
        "def traverse_directories3(directory):\n",
        "    cv = 0\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            full_file_path = os.path.join(root, filename)\n",
        "            R_val = int(filename.split('-')[2])-1\n",
        "            if R_val != 7:\n",
        "                if R_val != 0:\n",
        "                    R_val = R_val-1\n",
        "\n",
        "                R_filtered_filenames.append(full_file_path)\n",
        "                R_fname.append(filename)\n",
        "                R_labels.append(R_val)\n",
        "                R_emotion = EMOTIONS.get(R_val, 'Unknown')\n",
        "                R_emotion_labels.append(R_emotion)\n",
        "\n",
        "\n",
        "# Traverse through all subdirectories and extract file paths of English files\n",
        "traverse_directories3(directory_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "Mociju_z5OlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IEMOCAP**"
      ],
      "metadata": {
        "id": "I0Ug-RtG5PKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "''' angry, happy, sad, neutral, frustrated, excited, fearful, disgusted, excited, other\t'''\n",
        "\n",
        "EMOTIONS = {\n",
        "    0: 'neutral',\n",
        "    1: 'happy',\n",
        "    2: 'sad',\n",
        "    3: 'angry',\n",
        "    4: 'fear',\n",
        "    5: 'disgust',\n",
        "    # 7: 'surprised',\n",
        "    # 8: 'excited',\n",
        "    # 9: 'pleasure',\n",
        "    # 10: 'pain',\n",
        "    # 11: 'disappointment'\n",
        "}\n",
        "class CaseInsensitiveDict(dict):\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, str):\n",
        "            key = key.lower()\n",
        "        return super().__getitem__(key)\n",
        "\n",
        "IEM = CaseInsensitiveDict({\n",
        "    'neu': 0,\n",
        "    'hap': 1,\n",
        "    'sad': 2,\n",
        "    'ang': 3,\n",
        "    'fea': 4,\n",
        "    'dis': 5,\n",
        "    # 'exc': 1,\n",
        "    # anxious, apologetic, assertive, concerned, encouraging, excited\n",
        "})\n",
        "\n",
        "# Path to the main directory containing .wav files for five sessions\n",
        "sessions = [\n",
        "    r'D:\\Documents\\MASC\\Emo_rec_001\\IEMOCAP_DATA\\IEMOCAP_full_release_withoutVideos.tar\\IEMOCAP_full_release\\Session1\\sentences\\wav',\n",
        "    r'D:\\Documents\\MASC\\Emo_rec_001\\IEMOCAP_DATA\\IEMOCAP_full_release_withoutVideos.tar\\IEMOCAP_full_release\\Session2\\sentences\\wav',\n",
        "    r'D:\\Documents\\MASC\\Emo_rec_001\\IEMOCAP_DATA\\IEMOCAP_full_release_withoutVideos.tar\\IEMOCAP_full_release\\Session3\\sentences\\wav',\n",
        "    r'D:\\Documents\\MASC\\Emo_rec_001\\IEMOCAP_DATA\\IEMOCAP_full_release_withoutVideos.tar\\IEMOCAP_full_release\\Session4\\sentences\\wav',\n",
        "    r'D:\\Documents\\MASC\\Emo_rec_001\\IEMOCAP_DATA\\IEMOCAP_full_release_withoutVideos.tar\\IEMOCAP_full_release\\Session5\\sentences\\wav',\n",
        "]\n",
        "\n",
        "# Function to extract emotion labels, file directories, and file names for multiple sessions\n",
        "def extract_emotion_labels_and_files_for_sessions(sessions):\n",
        "    all_labels = []\n",
        "    all_directories = []\n",
        "    all_file_names = []\n",
        "    all_emos  = []\n",
        "\n",
        "    for session_dir in sessions:\n",
        "        wav_files = glob.glob(os.path.join(session_dir, '**', '*.wav'), recursive=True)\n",
        "        labels = []\n",
        "        file_directories = []\n",
        "        file_names = []\n",
        "        emos = []\n",
        "        count = 0\n",
        "\n",
        "        for wav_file in wav_files:\n",
        "            session_id = os.path.basename(os.path.dirname(wav_file))\n",
        "            file_directory = os.path.dirname(wav_file)\n",
        "            emo_eval_file = os.path.join(session_dir.replace('sentences\\\\wav', 'dialog\\\\EmoEvaluation'), f'{session_id}.txt')\n",
        "\n",
        "            if os.path.exists(emo_eval_file):\n",
        "                with open(emo_eval_file, 'r') as emo_file:\n",
        "                    lines = emo_file.readlines()\n",
        "                    file_name_with_extension = os.path.basename(wav_file)\n",
        "                    file_name = os.path.splitext(file_name_with_extension)[0]  # Extracting file name without extension\n",
        "\n",
        "                    for line_index, line in enumerate(lines):\n",
        "                        if file_name in line:\n",
        "                            emotion = line.split('\\t')[2].strip()\n",
        "                            if emotion == 'xxx':\n",
        "                                line_index+=1\n",
        "                                emotion = lines[line_index + 1].split('\\t')[1].strip()\n",
        "                                emotion = emotion[0:3]\n",
        "                                # count = count +1\n",
        "                                # if count <6:\n",
        "                                #     # print(line)\n",
        "                                #     print(emotion)\n",
        "                            lab = IEM.get(emotion, 12)\n",
        "                            if lab != 12:\n",
        "                                emo = EMOTIONS.get(lab, 'unknown')\n",
        "                                emos.append(emo)\n",
        "                                labels.append(lab)\n",
        "                                file_with_directory = os.path.join(file_directory, file_name_with_extension)\n",
        "                                file_directories.append(file_with_directory)\n",
        "                                file_names.append(file_name_with_extension)\n",
        "                            break  # Stop searching once emotion is found for the current .wav file\n",
        "\n",
        "        all_labels.extend(labels)\n",
        "        all_directories.extend(file_directories)\n",
        "        all_file_names.extend(file_names)\n",
        "        all_emos.extend(emos)\n",
        "\n",
        "    return all_labels, all_directories, all_file_names, all_emos\n",
        "\n",
        "# Extract emotion labels, file directories, and file names for multiple sessions\n",
        "emotion_labels, wav_file_directories, wav_file_names, all_emos = extract_emotion_labels_and_files_for_sessions(sessions)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1jYi1bid5YO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMBINE DATASETS**"
      ],
      "metadata": {
        "id": "7BBJmIxh5bZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FullPaths = T_filtered_filenames + C_filtered_filenames + R_filtered_filenames + JL_filtered_filenames + wav_file_directories\n",
        "FFNames =  T_fname + C_fname + R_fname + JL_fname + wav_file_names\n",
        "FLabels = T_labels + C_labels + R_labels + JL_labels + emotion_labels\n",
        "FFEmo = T_emotion_labels + C_emotion_labels + R_emotion_labels + JL_emotion_labels + all_emos\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'fname': FFNames,\n",
        "    'filename': FullPaths,\n",
        "    'label': FLabels,\n",
        "    'emotion_label': FFEmo\n",
        "})\n",
        "\n",
        "\n",
        "df.head (10)"
      ],
      "metadata": {
        "id": "IBahCE7Q5azt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['emotion_label'].value_counts()"
      ],
      "metadata": {
        "id": "WqnlCFgp5fKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.bar(x=range(6), height=df['emotion_label'].value_counts())\n",
        "ax.set_xticks(ticks=range(0, 6))\n",
        "ax.set_xticklabels([EMOTIONS[i] for i in range(0,6)], fontsize=10, rotation=45, ha='right') # Rotating labels by 45 degrees\n",
        "ax.set_xlabel('Emotions')\n",
        "ax.set_ylabel('Number of examples')"
      ],
      "metadata": {
        "id": "YiXrOuDw5kJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specify the test size (in this case, 20% for testing, 80% for training)\n",
        "test_size = 0.2\n",
        "\n",
        "# Split the DataFrame into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting DataFrames\n",
        "print(\"Train DataFrame shape:\", train_df.shape)\n",
        "print(\"Test DataFrame shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "FOAFzS6m5l4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PRE-PROCESSING**\n",
        "\n",
        "In this section we will remove noise from the signals and convert them to Mel-frequency cepstral coefficients (MFCC)"
      ],
      "metadata": {
        "id": "ct0YFspG5rKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing parameters\n",
        "sr = 44100 # Sampling rate\n",
        "duration = 5\n",
        "hop_length = 347 # to make time steps 128\n",
        "fmin = 20\n",
        "fmax = sr // 2\n",
        "n_mels = 128\n",
        "n_fft = n_mels * 20\n",
        "samples = sr * duration"
      ],
      "metadata": {
        "id": "eJ_KbEeT5t2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from tqdm.notebook import tqdm_notebook"
      ],
      "metadata": {
        "id": "ws7tPJH5570d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "import noisereduce as nr\n",
        "# load data\n",
        "rate, data = wavfile.read(\"D:\\Documents\\MASC\\Emo_rec_001\\TESS Toronto emotional speech set data\\OAF_angry\\OAF_back_angry.wav\")\n",
        "# perform noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
        "wavfile.write(\"mywav_reduced_noise.wav\", rate, reduced_noise)"
      ],
      "metadata": {
        "id": "HqgRBSTd59R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import librosa\n",
        "\n",
        "def read_audio(path, sr=44100, samples=44100):\n",
        "    '''\n",
        "    Reads in the audio file and returns\n",
        "    an array that we can turn into a melspectrogram\n",
        "    '''\n",
        "    y, sr = librosa.core.load(path, sr=sr)\n",
        "    #print (y.shape)\n",
        "    y = nr.reduce_noise(y=y, sr=sr)\n",
        "    #print (\"New Y\", y.shape)\n",
        "\n",
        "    # trim silence\n",
        "    if 0 < len(y): # workaround: 0 length causes error\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "\n",
        "    if len(y) > samples: # long enough\n",
        "        y = y[0:0+samples]\n",
        "    else: # pad blank\n",
        "        padding = samples - len(y)\n",
        "        offset = padding // 2\n",
        "        y = np.pad(y, (offset, samples - len(y) - offset), 'constant')\n",
        "\n",
        "    return y, sr\n",
        "\n",
        "def audio_to_melspectrogram(audio, sr, n_mels=128, hop_length=512, n_fft=2048, fmin=0, fmax=None):\n",
        "    '''\n",
        "    Convert to melspectrogram after audio is read in\n",
        "    '''\n",
        "    spectrogram = librosa.feature.melspectrogram(y=audio,\n",
        "                                                 sr=sr,\n",
        "                                                 n_mels=n_mels,\n",
        "                                                 hop_length=hop_length,\n",
        "                                                 n_fft=n_fft,\n",
        "                                                 fmin=fmin,\n",
        "                                                 fmax=fmax)\n",
        "    return librosa.power_to_db(spectrogram).astype(np.float32)\n",
        "\n",
        "def read_as_melspectrogram(path):\n",
        "    '''\n",
        "    Convert audio into a melspectrogram\n",
        "    so we can use machine learning\n",
        "    '''\n",
        "    afile, sr = read_audio(path)\n",
        "    mels = audio_to_melspectrogram(afile, sr = sr)\n",
        "    return mels\n",
        "\n",
        "def convert_wav_to_image(df):\n",
        "    X = []\n",
        "    for _,row in tqdm_notebook(df.iterrows()):\n",
        "        file_path = row['filename']\n",
        "        print(file_path)\n",
        "        x = read_as_melspectrogram(file_path)\n",
        "        X.append(x.transpose())\n",
        "    return X\n",
        "\n",
        "def convert_wav_to_image2(df):\n",
        "    X = []\n",
        "    for _, row in df.iterrows():\n",
        "        file_path = row['filename']\n",
        "        # print(file_path)\n",
        "        x = read_as_melspectrogram(file_path)\n",
        "        X.append(x.transpose())\n",
        "    return X\n",
        "\n",
        "\n",
        "def normalize(img):\n",
        "    '''\n",
        "    Normalizes an array\n",
        "    (subtract mean and divide by standard deviation)\n",
        "    '''\n",
        "    eps = 0.001\n",
        "    if np.std(img) != 0:\n",
        "        img = (img - np.mean(img)) / np.std(img)\n",
        "    else:\n",
        "        img = (img - np.mean(img)) / eps\n",
        "    return img\n",
        "\n",
        "def normalize_dataset(X):\n",
        "    '''\n",
        "    Normalizes list of arrays\n",
        "    (subtract mean and divide by standard deviation)\n",
        "    '''\n",
        "    normalized_dataset = []\n",
        "    for img in X:\n",
        "        normalized = normalize(img)\n",
        "        normalized_dataset.append(normalized)\n",
        "    return normalized_dataset"
      ],
      "metadata": {
        "id": "ymuc3OzL5-aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = r'D:\\Documents\\MASC\\Emo_rec_001\\SAVED_SETS\\LSTM_RAVDESS_CREMAD'\n",
        "\n",
        "file_paths = train_df['filename'].tolist()\n",
        "\n",
        "# Preprocess dataset and create validation sets\n",
        "X = np.array(convert_wav_to_image(train_df))  # Assuming you have a function convert_wav_to_image\n",
        "X = normalize_dataset(X)\n",
        "Y = train_df['label'].values\n",
        "\n",
        "np.save(os.path.join(save_directory, 'X_Tr008.npy'), X)\n",
        "np.save(os.path.join(save_directory, 'Y_Tr008.npy'), Y)\n",
        "\n",
        "X_test = np.array(convert_wav_to_image(test_df))  # Assuming you have a function convert_wav_to_image\n",
        "X_test = normalize_dataset(X_test)\n",
        "Y_test = test_df['label'].values\n",
        "\n",
        "\n",
        "np.save(os.path.join(save_directory, 'X_Te008.npy'), X_test)\n",
        "np.save(os.path.join(save_directory, 'Y_Te008.npy'), Y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "B885PgiH6BG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}